---
layout: post
title: 使用tf-idf算法实现文本相似度
date: 2018-08-30
tag: 机器学习
---


#TF-IDF算法实现文本相似度

> tf-idf是一种统计方法，通常用来评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
> 
> 字词的重要性伴随着它在文件中出现的次数成正比，但同时会伴随着它在语料库中的出现的频率成反比下降
> 
> TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

* 一个栗子：

	在某个一共有一千词的网页中“原子能”、“的”和“应用”分别出现了 2 次、35 次 和 5 次，那么它们的词频就分别是 0.002、0.035 和 0.005。 我们将这三个数相加，其和 0.042 就是相应网页和查询“原子能的应用” 相关性的一个简单的度量。概括地讲，如果一个查询包含关键词 w1,w2,...,wN, 它们在一篇特定网页中的词频分别是: TF1, TF2, ..., TFN。 （TF: term frequency)。 那么，这个查询和该网页的相关性就是:TF1 + TF2 + ... + TFN。
	读者可能已经发现了又一个漏洞。在上面的例子中，词“的”占了总词频的 80% 以上，而它对确定网页的主题几乎没有用。我们称这种词叫“应删除词”（Stopwords)，也就是说在度量相关性是不应考虑它们的频率。在汉语中，应删除词还有“是”、“和”、“中”、“地”、“得”等等几十个。忽略这些应删除词后，上述网页的相似度就变成了0.007，其中“原子能”贡献了 0.002，“应用”贡献了 0.005。细心的读者可能还会发现另一个小的漏洞。在汉语中，“应用”是个很通用的词，而“原子能”是个很专业的词，后者在相关性排名中比前者重要。因此我们需要给汉语中的每一个词给一个权重，这个权重的设定必须满足下面两个条件：
	1. 一个词预测主题能力越强，权重就越大，反之，权重就越小。我们在网页中看到“原子能”这个词，或多或少地能了解网页的主题。我们看到“应用”一次，对主题基本上还是一无所知。因此，“原子能“的权重就应该比应用大。
	2. 应删除词的权重应该是零。

### 理论知识简单说说后，接下来着重精力放在代码中，先来看我们要比较相似的文本。


* 代码编写步骤：

    1. 读取文档
    2. 对要计算的文档进行分词
    3. 对文档进行整理成指定格式，方便后续进行计算
    4. 计算出词语的频率
    5. [可选]UI频率低的词语进行过滤
    6. 通过语料库建立词典
    7. 加载要对比的文档
    8. 将要对比的文档通过doc2bow转化为稀疏向量
    9. 对稀疏向量进行进一步处理得到新的语料库
    10. 对新语料库通过tfidf进行处理，得到tfidf
    11. 得到token2id的特征数
    12. 稀疏矩阵的相似度，从而建立索引
    13. 得到最终相似度结果	
    
* 素材

	这是百度百科关于[史蒂夫乔布斯的简介](https://baike.baidu.com/item/%E5%8F%B2%E8%92%82%E5%A4%AB%C2%B7%E4%B9%94%E5%B8%83%E6%96%AF/85300?fr=aladdin)，我们拿图片中的两个内容做训练集
	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-5/43611801.jpg)
	
	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-5/18080880.jpg)
	
	
	下面的这张图片是[搜狗百科对乔布斯](https://baike.sogou.com/v43650144.htm?fromTitle=%E5%8F%B2%E8%92%82%E5%A4%AB%E4%B9%94%E5%B8%83%E6%96%AF)的描述，我们那这部分内容做测试集
	
	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-5/1852856.jpg)

* 代码展示

		'''
		将百度百科的两个内容，以文本方式保存在sample.txt和training.txt两个文本中
		将搜狗百科的内容作为测试集保存在了test.txt文件中
		
		'''
		# 语料库 模型和相似度
		from gensim import corpora, models, similarities
		import jieba
		from collections import defaultdict
		
		
		# sample和trainging两个都是训练集
		# test是测试集
		sample = 'sample.txt'
		training = "training.txt"
		# 读取这两个样本集文档
		sampleContent = open(sample, "rb").read().decode("utf-8", 'ignore')
		trainingContent = open(training, "rb").read().decode("utf-8", 'ignore')
		# 对文档进行分词操作
		simpleList = jieba.cut(sampleContent)
		trainingList = jieba.cut(trainingContent)
		
		# 在分词后对词语进行处理，方便后面操作
		# 单词1 单词2 单词3 ...单词n
		simpleWords = ""
		for word in simpleList:
		    simpleWords += word + " "
		# print(simpleWords)
		
		trainingWords = ""
		for word in trainingList:
		    trainingWords += word + " "
		# print(trainingWords)
		# 将处理后的词语存放在document的list中
		documents = [simpleWords, trainingWords]
		texts = [[word for word in document.split()]
		         for document in documents]
		
		# 计算分词后出现词语的个数（词频）（仅在筛选时有用）
		frequence = defaultdict(int)
		for text in texts:
		    for token in text:
		        frequence[token] += 1
		# print(frequence)
		# 频率低的词语进行过滤
		texts = [[word for word in text if frequence[token] > 3]
		         for text in texts]
		print(texts)
		# 通过语料库建立词典
		dictionary = corpora.Dictionary(texts)
		dictionary.save("dict.txt")
		# 对测试集进行读取
		doc = "test.txt"
		testContent3 = open(doc, 'rb').read().decode("utf-8", 'ignore')
		# 对测试集进行分词
		testCut = jieba.cut(testContent3)
		testDictStr = ""
		for item in testCut:
		    testDictStr += item + " "
		# 将样本集转换为稀疏向量
		new_vec = dictionary.doc2bow(testDictStr.split())
		# 将训练集转换成稀疏向量
		corpus = [dictionary.doc2bow(text) for text in texts]
		tfidf = models.TfidfModel(corpus)
		featureNum = len(dictionary.token2id.keys())
		index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=featureNum)
		similary = index[tfidf[new_vec]]
		string_tfidf = tfidf[new_vec]
		print(similary)
		print(new_vec)
		print(string_tfidf)

----------
[github代码](https://github.com/apodxx/text_similarity)

[部分内容参考一](https://blog.csdn.net/duinodu/article/details/76618638)

[部分内容参考二](https://radimrehurek.com/gensim/similarities/docsim.html)	
		
		

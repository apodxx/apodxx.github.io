---
layout: post
title: Linux系列教程:RAID
date: 2018-2-18
tag: Linux系列教程
---

> 今天我们对`RAID`进行简单学习，如果你有什么好的的建议和想法，欢迎在下方评论，需要说明的是本教程的所有内容笔记绝大部分来自刘遄的[《linux就该这么学》](http://www.linuxprobe.com/book)，在这里向刘遄老师表示感谢！


<h3>什么是RAID?</h3>
RAID（Redundant Array of Independent Disks，独立冗余磁盘阵列）:RAID 技术
通过把多个硬盘设备组合成一个容量更大、安全性更好的磁盘阵列，并把数据切割成多个区
段后分别存放在各个不同的物理硬盘设备上，然后利用分散读写技术来提升磁盘阵列整体的
性能，同时把多个重要数据的副本同步到不同的物理硬盘设备上，从而起到了非常好的数据
冗余备份效果,RAID 使用比较广泛的是RAID0，RAID1，RAID5 和 RAID10

<h3>RAID0</h3>
RAID 0 技术把多块物理硬盘设备（至少两块）通过硬件或软件的方式串联在一起，组成
一个大的卷组，并将数据依次写入到各个物理硬盘中。这样一来，在最理想的状态下，硬盘设
备的读写性能会提升数倍，但是若任意一块硬盘发生故障将导致整个系统的数据都受到破坏。
通俗来说， RAID 0 技术能够有效地提升硬盘数据的吞吐速度，但是不具备数据备份和错误修
复能力。如图 7-1 所示，数据被分别写入到不同的硬盘设备中，即 disk1 和 disk2 硬盘设备会
分别保存数据资料，最终实现分开写入、读取的效果。![](http://p0kzdnfmg.bkt.clouddn.com/18-1-23/57690367.jpg)
<h3>RAID1</h3>
尽管 RAID 0 技术提升了硬盘设备的读写速度，但是它是将数据依次写入到各个物理硬
盘中，也就是说，它的数据是分开存放的，其中任何一块硬盘发生故障都会损坏整个系统的
数据。因此，如果生产环境对硬盘设备的读写速度没有要求，而是希望增加数据的安全性时，
就需要用到 RAID 1 技术了。
在图 7-2 所示的 RAID 1 技术示意图中可以看到，它是把两块以上的硬盘设备进行绑
定，在写入数据时，是将数据同时写入到多块硬盘设备上（可以将其视为数据的镜像或备
份）。当其中某一块硬盘发生故障后，一般会立即自动以热交换的方式来恢复数据的正常使用。
![](http://p0kzdnfmg.bkt.clouddn.com/18-1-23/85566441.jpg)
RAID 1 技术虽然十分注重数据的安全性，但是因为是在多块硬盘设备中写入了相同的数据，因此硬盘设备的利用率得以下降，从理论上来说，图 7-2 所示的硬盘空间的真实可用率只
有 50%，由三块硬盘设备组成的 RAID 1 磁盘阵列的可用率只有 33%左右，以此类推。而且，
由于需要把数据同时写入到两块以上的硬盘设备，这无疑也在一定程度上增大了系统计算功
能的负载。
<h3>RAID5</h3>
 RAID5 技术是把硬盘设备的数据奇偶校验信息保存到其他硬盘设备中。
RAID 5 磁盘阵列组中数据的奇偶校验信息并不是单独保存到某一块硬盘设备中，而是存储到
除自身以外的其他每一块硬盘设备上，这样的好处是其中任何一设备损坏后不至于出现致命
缺陷；图 7-3 中 parity 部分存放的就是数据的奇偶校验信息，换句话说，就是 RAID 5 技术实
际上没有备份硬盘中的真实数据信息，而是当硬盘设备出现问题后通过奇偶校验信息来尝试
重建损坏的数据。 RAID 这样的技术特性“妥协”地兼顾了硬盘设备的读写速度、数据安全性与存储成本问题
![](http://p0kzdnfmg.bkt.clouddn.com/18-1-23/58326361.jpg)
<h3>RAID10</h3>
大部分企业更在乎的是数据本身的价值而非硬盘价格，因此生产环境中主要使
用 RAID 10 技术。
RAID 10 技术是 RAID 1+RAID 0 技术的一个“组合体”。如图 7-4 所示，
RAID 10 技术需要至少 4 块硬盘来组建，其中先分别两两制作成 RAID 1 磁盘阵列，以保
证数据的安全性；然后再对两个 RAID 1 磁盘阵列实施 RAID 0 技术，进一步提高硬盘设
备的读写速度。这样从理论上来讲，只要坏的不是同一组中的所有硬盘，那么最多可以损
坏 50%的硬盘设备而不丢失数据。由于 RAID 10 技术继承了 RAID 0 的高读写速度和 RAID
1 的数据安全性，在不考虑成本的情况下 RAID 10 的性能都超过了 RAID 5，因此当前成
为广泛使用的一种存储技术。

![](http://p0kzdnfmg.bkt.clouddn.com/18-1-23/67320523.jpg)

**[点击观看介绍关于RAID各级别的视频](http://www.bilibili.com/video/av13557543)**

<h3>部署RAID10磁盘阵列</h3>
在部署前，需要用到`mdadm`命令，用于管理Linux系统中的软件RAID硬件阵列，格式`mdadm [模式] <RAID设备名称> [选项] [成员设备名称]`
![](http://p0kzdnfmg.bkt.clouddn.com/18-1-23/11286216.jpg)
下面开始具体的步骤：

1. 首先在VirtualBox添加四块磁盘(RAID10的最低要求为4个)
 ![](http://p0kzdnfmg.bkt.clouddn.com/18-1-23/58587743.jpg)
2. 开机备份(这个是为后面的部署RAID5做准备，方便后面操作)，开机后，“控制” -> “生成备份[系统快照]”
 ![](http://p0kzdnfmg.bkt.clouddn.com/18-1-23/6572778.jpg)
3. 我们进入到`dev`目录中，可以看到我们添加的四块硬盘即为sdb，sdc，sdd，sde设备文件

		[root@apodx dev]# ls /dev | more
		...
		sda
		sda1
		sda2
		sdb
		sdc
		sdd
		sde
		...
4. 接下来，我们正式开始...

		[root@apodx ~]# mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/sdb /dev/sdc /dev/sdd /dev/sde
		mdadm: layout defaults to n2
		mdadm: layout defaults to n2
		mdadm: chunk size defaults to 512K
		mdadm: size set to 20955136K
		mdadm: Defaulting to version 1.2 metadata
		mdadm: array /dev/md0 started.

	其中， -C 参数代表创建一个 RAID 阵列卡； -v 参
	数显示创建的过程，同时在后面追加一个设备名称/dev/md0，这样/dev/md0就是创建后的 RAID
	磁盘阵列的名称； -a yes 参数代表自动创建设备文件； -n 4 参数代表使用 4 块硬盘来部署这个
	RAID 磁盘阵列；而-l 10 参数则代表 RAID 10 方案；最后再加上 4 块硬盘设备的名称就搞定
	了。
5. 将制作好的RAID磁盘阵列格式化为ext4格式

		[root@apodx ~]# mkfs.ext4 /dev/md0 
		mke2fs 1.42.9 (28-Dec-2013)
		Filesystem label=
		OS type: Linux
		Block size=4096 (log=2)
		Fragment size=4096 (log=2)
		Stride=128 blocks, Stripe width=256 blocks
		2621440 inodes, 10477568 blocks
		523878 blocks (5.00%) reserved for the super user
		First data block=0
		Maximum filesystem blocks=2157969408
		320 block groups
		32768 blocks per group, 32768 fragments per group
		8192 inodes per group
		Superblock backups stored on blocks: 
			32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
			4096000, 7962624
		
		Allocating group tables: done                            
		Writing inode tables: done                            
		Creating journal (32768 blocks): done
		Writing superblocks and filesystem accounting information: done 
6. 使用mount命令挂载(临时挂载)

	[root@apodx /]# mount /dev/md0 /RAID
	
7. 接下来就是创建挂载点然后把磁盘设备进行挂在操作(永久挂载)。

		echo "/dev/md0 /RAID ext4 defaults 0 0" >> /etc/fstab
8. 重启可以看到RAID磁盘阵列已经成功挂载	

		[root@apodx ~]# df -h
		Filesystem               Size  Used Avail Use% Mounted on
		/dev/mapper/centos-root   37G  4.0G   34G  11% /
		devtmpfs                 905M     0  905M   0% /dev
		tmpfs                    920M     0  920M   0% /dev/shm
		tmpfs                    920M  8.9M  911M   1% /run
		tmpfs                    920M     0  920M   0% /sys/fs/cgroup
		/dev/sda1               1014M  216M  799M  22% /boot
		/dev/md0                  40G   49M   38G   1% /RAID
		tmpfs                    184M  4.0K  184M   1% /run/user/42
		tmpfs                    184M   20K  184M   1% /run/user/1000
		/dev/sr0                  57M   57M     0 100% /run/media/apodx/VBOXADDITIONS_5.1.30_118389
<h3>损坏磁盘阵列及修复</h3>
在确认有一块物理硬盘设备出现损坏而不能继续正常使用后，应该使用 mdadm 命令将其
移除，然后查看 RAID 磁盘阵列的状态，可以发现状态已经改变。

		[root@apodx /]# mdadm /dev/md0 -f /dev/sdb
		mdadm: set /dev/sdb faulty in /dev/md0
		[root@apodx /]# mdadm -D /dev/md0
		/dev/md0:
		           Version : 1.2
		     Creation Time : Tue Jan 23 19:36:14 2018
		        Raid Level : raid10
		        Array Size : 41910272 (39.97 GiB 42.92 GB)
		     Used Dev Size : 20955136 (19.98 GiB 21.46 GB)
		      Raid Devices : 4
		     Total Devices : 4
		       Persistence : Superblock is persistent
		
		       Update Time : Tue Jan 23 20:19:45 2018
		             State : clean, degraded 
		    Active Devices : 3
		   Working Devices : 3
		    Failed Devices : 1
		     Spare Devices : 0
		
		            Layout : near=2
		        Chunk Size : 512K
		
		Consistency Policy : resync
		
		              Name : apodx:0  (local to host apodx)
		              UUID : 0b1187a5:9ab2041d:d35bb9d2:7b0aacd3
		            Events : 21
		
		    Number   Major   Minor   RaidDevice State
		       -       0        0        0      removed
		       1       8       32        1      active sync set-B   /dev/sdc
		       2       8       48        2      active sync set-A   /dev/sdd
		       3       8       64        3      active sync set-B   /dev/sde
		
		       0       8       16        -      faulty   /dev/sdb
	
			[root@apodx /]# umount /RAID/
卸载挂载目录后，然后重启，我们在重新把sdb设备添加进入，新的硬盘有重新添加到了RAID磁盘阵列中

		[root@apodx ~]# mdadm /dev/md0 -a /dev/sdb
		mdadm: added /dev/sdb
		[root@apodx ~]# mdadm -D /dev/md0
		/dev/md0:
		           Version : 1.2
		     Creation Time : Tue Jan 23 19:36:14 2018
		        Raid Level : raid10
		        Array Size : 41910272 (39.97 GiB 42.92 GB)
		     Used Dev Size : 20955136 (19.98 GiB 21.46 GB)
		      Raid Devices : 4
		     Total Devices : 4
		       Persistence : Superblock is persistent
		
		       Update Time : Tue Jan 23 21:42:11 2018
		             State : clean, degraded, recovering 
		    Active Devices : 3
		   Working Devices : 4
		    Failed Devices : 0
		     Spare Devices : 1
		
		            Layout : near=2
		        Chunk Size : 512K
		
		Consistency Policy : resync
		
		    Rebuild Status : 18% complete
		
		              Name : apodx:0  (local to host apodx)
		              UUID : 0b1187a5:9ab2041d:d35bb9d2:7b0aacd3
		            Events : 29
		
		    Number   Major   Minor   RaidDevice State
		       4       8       16        0      spare rebuilding   /dev/sdb
		       1       8       32        1      active sync set-B   /dev/sdc
		       2       8       48        2      active sync set-A   /dev/sdd
		       3       8       64        3      active sync set-B   /dev/sde
	
<h3>RAID5磁盘阵列+备份盘</h3>
上面已经把RAID10演示完毕，下面就是对RAID5的操作，前面说到的RAID5至少需要3块硬盘 在这里我们还需要一块备份硬盘。

1. 首先把系统恢复到RAID10时设置的还原点。
2. 现在创建一个 RAID 5 磁盘阵列+备份盘。在下面的命令中，参数-n 3 代表创建这个
RAID 5 磁盘阵列所需的硬盘数，参数-l 5 代表 RAID 的级别，而参数-x 1 则代表有一块备
份盘。当查看/dev/md0（即 RAID 5 磁盘阵列的名称）磁盘阵列的时候就能看到有一块备
份盘在等待中了。

		[root@apodx dev]# mdadm -Cv /dev/md0 -n 3 -l 5 -x 1 /dev/sdb /dev/sdc /dev/sdd /dev/sde
		mdadm: layout defaults to left-symmetric
		mdadm: layout defaults to left-symmetric
		mdadm: chunk size defaults to 512K
		mdadm: size set to 20955136K
		mdadm: Defaulting to version 1.2 metadata
		mdadm: array /dev/md0 started.

		[root@apodx dev]# mdadm -D /dev/md0
		/dev/md0:
		           Version : 1.2
		     Creation Time : Tue Jan 23 22:35:22 2018
		        Raid Level : raid5
		        Array Size : 41910272 (39.97 GiB 42.92 GB)
		     Used Dev Size : 20955136 (19.98 GiB 21.46 GB)
		      Raid Devices : 3
		     Total Devices : 4
		       Persistence : Superblock is persistent
		
		       Update Time : Tue Jan 23 22:35:42 2018
		             State : clean, degraded, recovering 
		    Active Devices : 2
		   Working Devices : 4
		    Failed Devices : 0
		     Spare Devices : 2
		
		            Layout : left-symmetric
		        Chunk Size : 512K
		
		Consistency Policy : resync
		
		    Rebuild Status : 22% complete
		
		              Name : apodx:0  (local to host apodx)
		              UUID : 01b8f5bc:8ae63733:4764eb51:51db0c14
		            Events : 4
		
		    Number   Major   Minor   RaidDevice State
		       0       8       16        0      active sync   /dev/sdb
		       1       8       32        1      active sync   /dev/sdc
		       4       8       48        2      spare rebuilding   /dev/sdd
		
		       3       8       64        -      spare   /dev/sde
3. 格式化RAID5磁盘阵列为ext4文件格式

		[root@apodx dev]# mkfs.ext4 /dev/md0
		mke2fs 1.42.9 (28-Dec-2013)
		Filesystem label=
		OS type: Linux
		Block size=4096 (log=2)
		Fragment size=4096 (log=2)
		Stride=128 blocks, Stripe width=256 blocks
		2621440 inodes, 10477568 blocks
		523878 blocks (5.00%) reserved for the super user
		First data block=0
		Maximum filesystem blocks=2157969408
		320 block groups
		32768 blocks per group, 32768 fragments per group
		8192 inodes per group
		Superblock backups stored on blocks: 
			32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
			4096000, 7962624
		
		Allocating group tables: done                            
		Writing inode tables: done                            
		Creating journal (32768 blocks): done
		Writing superblocks and filesystem accounting information: done   
4. 接下来模拟sdb磁盘损坏，然后查看磁盘阵列有什么变化

		[root@apodx dev]# mdadm /dev/md0 -f /dev/sdb
		mdadm: set /dev/sdb faulty in /dev/md0
		[root@apodx dev]# mdadm -D/dev/md0
		mdadm: invalid option -- '/'
		Usage: mdadm --help
		  for help
		[root@apodx dev]# mdadm -D /dev/md0
		/dev/md0:
		           Version : 1.2
		     Creation Time : Tue Jan 23 22:35:22 2018
		        Raid Level : raid5
		        Array Size : 41910272 (39.97 GiB 42.92 GB)
		     Used Dev Size : 20955136 (19.98 GiB 21.46 GB)
		      Raid Devices : 3
		     Total Devices : 4
		       Persistence : Superblock is persistent
		
		       Update Time : Tue Jan 23 22:40:36 2018
		             State : clean, degraded, recovering 
		    Active Devices : 2
		   Working Devices : 3
		    Failed Devices : 1
		     Spare Devices : 1
		
		            Layout : left-symmetric
		        Chunk Size : 512K
		
		Consistency Policy : resync
		
		    Rebuild Status : 16% complete
		
		              Name : apodx:0  (local to host apodx)
		              UUID : 01b8f5bc:8ae63733:4764eb51:51db0c14
		            Events : 22
		
		    Number   Major   Minor   RaidDevice State
		       3       8       64        0      spare rebuilding   /dev/sde
		       1       8       32        1      active sync   /dev/sdc
		       4       8       48        2      active sync   /dev/sdd
		
		       0       8       16        -      faulty   /dev/sdb
		
由上面的代码可以看到 当我们把硬盘设备/dev/sdb 移出磁盘阵列，然后迅速查看
/dev/md0 磁盘阵列的状态，就会发现备份盘已经被自动顶替上去并开始了数据同步。
---
layout: post
title: 决策树中的交叉验证
date: 2021-02-04
tag: 机器学习
---
### 为什么需要交叉验证

解决随机划分的差异


```python
from sklearn.model_selection import GridSearchCV
```


```python
values = np.linspace(0,0.5,50)
depths = range(2,15)
```


```python
param_grid = {"max_depth":depths,'min_impurity_split':values}
```


```python
#GridSearchCV，它存在的意义就是自动调参，只要把参数输进去，就能给出最优化的结果和参数。
#但是这个方法适合于小数据集，一旦数据的量级上去了，很难得出结果。
model = GridSearchCV(DecisionTreeClassifier(),param_grid,cv=5)
```


```python
x_train.shape,data_target.shape
```




    ((712, 11), (891, 1))




```python
y = [x[0] for x in data_target]
```


```python
y_train = pd.Series(y)
```


```python
model.fit(data_train,y_train)
```




    GridSearchCV(cv=5, error_score='raise',
           estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
                max_features=None, max_leaf_nodes=None,
                min_impurity_split=1e-07, min_samples_leaf=1,
                min_samples_split=2, min_weight_fraction_leaf=0.0,
                presort=False, random_state=None, splitter='best'),
           fit_params={}, iid=True, n_jobs=1,
           param_grid={'max_depth': range(2, 15), 'min_impurity_split': array([ 0.     ,  0.0102 ,  0.02041,  0.03061,  0.04082,  0.05102,
            0.06122,  0.07143,  0.08163,  0.09184,  0.10204,  0.11224,
            0.12245,  0.13265,  0.14286,  0.15306,  0.16327,  0.17347,
            0.18367,  0.19388,  0.20408,  ...837,
            0.42857,  0.43878,  0.44898,  0.45918,  0.46939,  0.47959,
            0.4898 ,  0.5    ])},
           pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
           scoring=None, verbose=0)




```python
model.best_params_
```




    {'max_depth': 9, 'min_impurity_split': 0.18367346938775508}




```python
model.best_score_
```




    0.83052749719416386




```python

```
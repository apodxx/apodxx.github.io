---
layout: post
title: 梯度下降算法及Python代码实现
date: 2018-09-01
tag: 机器学习
---

# 梯度下降法

----------


梯度下降法是机器学习中使用非常广泛的算法，主要的作用对损失函数提供优化

举个例子来说，我们在下山时，便可以看作是梯度下降，目标是到山下，下山的方法有多种多样的，不同的地方到山底方法也是多种多样。

如果大家想具体的了解梯度下降的算法思想，可以去B站看[Andrew Ng的梯度学习那几节](https://www.bilibili.com/video/av9912938/?p=7)
从损失函数到梯度下降，学完后再结合这篇文章食用效果更佳。

下面我们先来梳理几个数学公式：

1. 先看预测函数`h(x)`：

	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-4/30663876.jpg)
	
	* h(x)是预测结果
	* x1 , x2 为特征向量，也就是对决定预测结果的每个特征
	* sita0为偏移量，你可以看成是线性方程的截距，eg：y = kx + b 中的 b
	* sita1 , sita2 他们是特征向量的权重，确定好合适的权重才能得到正确的结果 
	* 为何进一步简写这个函数，也可以写成
	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-4/22556472.jpg)


2. 在看下损失函数`J(sita)`:
	
	

	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-4/86314541.jpg)

	* 损失函数(cost function)，在这里我们采用的是最小二乘法的算法
	* h(x)表示的便是我们上面的预测值，y(x)是真实值
	* 前面的1/2为何后面的求解偏导数时，可以与二次方抵消
	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-4/59221716.jpg)
	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-4/57671278.jpg)
 
3. 梯度下降算法:

	![](http://p0kzdnfmg.bkt.clouddn.com/18-9-4/22622589.jpg)

	* 这个是梯度方法的核心，`sita j` 需要我们在编程时给它初始值，你可以想象成我们在下山时，都会以我们当前的位置设置下山路线，而初始值便是下山的当前位置
	* alpha 成为学习率，你可以理解为步长，表示我们在下山时，走的一步，所以夸张的讲，你的步子不能太大，也不能太小，太大可能跨到另一座山上，如果太小像个蚂蚁，何时能到山底啊，所以选择一个合适的学习率是非常重要的
	* 最后的那一点，便是对损失函数求偏导，再加上负号，他的实际意义便是保证我们的每一步都能朝正确的方向前进。

有了上面的知识储备，最后我们在结合代码来看看吧！先来看一个简单的，当特征向量只有一个的时候，也就是h(x)为一元一次方程的时候。

	import matplotlib.pyplot as plt
	#数据是我自己编的，故意编的很像一个单调递增的一次函数
	x = [100,80,120,75,60,43,140,132,63,55,74,44,88]
	y = [120,92,143,87,60,50,167,147,80,60,90,57,99]
	
	#步长
	alpha = 0.00001
	
	cnt = 0
	#计算样本个数
	m = len(x)
	#初始化参数的值，拟合函数为 h=theta0+theta1*x
	theta0 = 0
	theta1 = 0
	#误差
	error0=0
	error1=0
	#退出迭代的两次误差差值的阈值
	epsilon=0.000001
	theta0_history=[]
	theta1_history=[]
	
	def h(x):
	    return theta0+theta1*x
	#开始迭代
	while True:
	    diff = [0, 0]
	    for i in range(len(x)):
	# 求J(theta)导数
	        diff[0]+=(theta0+theta1*x[i]-y[i])
	        diff[1]+=(theta0+theta1*x[i]-y[i])*x[i]
	 
	# 计算梯度下降算法
	    theta0 -= alpha*diff[0]
	    theta1 -= alpha*diff[1]
	    theta0_history.append(theta0)
	    theta1_history.append(theta1)
	    error1=0
	# 当损失函数的结果差值最终小于阈值，基本到达山底
	    for i in range(len(x)):
	        error1+=(y[i]-theta0-theta1*x[i])**2/2
	    if abs(error1-error0)<epsilon:
	        break
	    else:
	        error0=error1
	
	plt.plot(x,y,'bo')
	plt.plot(x,[h(x1) for x1 in x])
	print("theta0_history:"+str(theta0_history)+"\ntheta1_history:"+str(theta1_history))
	print("thetal0:"+str(theta0)+"\ntheta1"+str(theta1))
	plt.show()

结果：
![](http://p0kzdnfmg.bkt.clouddn.com/18-9-4/92253767.jpg)

当有两个特征的时候，我们再来使用梯度下降来拟合预测函数



	x = [(1, 0., 3),
	     (1, 1., 3),
	     (1, 3., 2),
	     (1, 4., 4)]
	# y = [95.364, 97.217205, 75.195834, 60.105519, 49.342380]
	y = [10,
	     12,
	     13,
	     21]
	eps = 0.00000001
	
	alpha = 0.01
	diff = [0, 0, 0]
	max_itor = 1000
	error1 = 0
	error0 = 0
	count = 0
	m = len(x)
	
	theta0 = 0
	theta1 = 0
	theta2 = 0
	
	while True:
	    count += 1
	    # 更新梯度
	    diff[0] = diff[1] = diff[2] = 0
	    for i in range(m):  # += 是因为这是对所有样本进行一次更新，J(θ)和随机梯度下降是不一样的
	        diff[0] += ((theta0 + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]) * 1
	        diff[1] += ((theta0 + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]) * x[i][1]
	        diff[2] += ((theta0 + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]) * x[i][2]
	
	    # 更新梯度 for all j simultaneously
	    theta0 -= alpha * diff[0]
	    theta1 -= alpha * diff[1]
	    theta2 -= alpha * diff[2]
	
	    error1 = 0
	    for i in range(m):
	        error1 += 1 / 2 * (y[i] - theta0 - theta1 * x[i][1] - theta2 * x[i][2]) ** 2
	    if abs(error1 - error0) < eps:
	        break
	    else:
	        error0 = error1
	
	    # print(' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f' % (theta0, theta1, theta2, error1))
	    print('Done: theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2))
	    print('迭代次数: %d' % count)
	
结果为

	0.941162386559514 2.0776180393756896 2.958033408668362

[相关代码参考](https://blog.csdn.net/c369624808/article/details/78414746)
[github代码下载](https://github.com/apodxx/gradient_decent)

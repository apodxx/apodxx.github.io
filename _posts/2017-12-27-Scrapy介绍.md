---
layout: post
title: Scrapy的介绍
date: 2016-12-27
tag: Scrapy系列教程
---



* 什么是Scrapy ?

> Scrapy，Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和[自动化测试](https://baike.baidu.com/item/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95)。
>
> Scrapy吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。它也提供了多种类型爬虫的基类，如BaseSpider、sitemap爬虫等，最新版本又提供了web2.0爬虫的支持。
>
> Scrap，是碎片的意思，这个Python的爬虫框架叫Scrapy。    (摘自百度百科)



* Scrapy的特点
  1. 基于框架,scrapy对千万级URL去重支持很好,适合爬取海量的数据内容
  2. 适合与爬取静态网页,对于动态网页不适合(不是说不行)



首先我们先看下Scrapy的架构图:



![](http://p0kzdnfmg.bkt.clouddn.com/17-12-23/44520710.jpg)



- `Scrapy Engine(引擎)`: 如果把 Scrapy 比作一个蚂蚁窝,那么这个`Scrapy Engine` 就是蚁后 它就是最大的 boss ,负责`Spider`、`ItemPipeline`、`Downloader`、`Scheduler`中间的通讯，信号、数据传递等
- `Scheduler(调度器)`: 它负责接受`引擎`发送过来的Request请求，并按照一定的方式进行整理排列，入队，当`引擎`需要时，交还给`引擎`。
- `Downloader（下载器）`：负责下载`Scrapy Engine(引擎)`发送的所有Requests请求，并将其获取到的Responses交还给`Scrapy Engine(引擎)`，由`引擎`交给`Spider`来处理，
- `Spider（爬虫）`：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给`引擎`，再次进入`Scheduler(调度器)`，还是上面的例子,`Spider`就相当工蚁,所以的粗活累活都被他承包了 ,他负责去寻找食物(即我们要爬取的网页)
- `Item Pipeline(管道)`：它负责处理`Spider`中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.
- `Downloader Middlewares（下载中间件）`：你可以当作是一个可以自定义扩展下载功能的组件。
- `Spider Middlewares（Spider中间件）`：你可以理解为是一个可以自定扩展和操作`引擎`和`Spider`中间`通信`的功能组件（比如进入`Spider`的Responses;和从`Spider`出去的Requests）





## Scrapy系列教程(二):Scrapy的安装

 教程一简单的说明了什么是 Scrapy ,相比各位小伙伴已经手痒痒了想试试,废话少说 我们马上开始~~~

安装前的准备:

1. 系统为Linux(CentOS)或者Mac,Windows请飘过….

2. 安装及配置`pip`及配置`pip`源

   * 安装`python` ,本次安装的版本是2.7,如果你想安装3.x版本也可以(早前Scrapy是不支持3.x的,16年2月4号开始支持3.x)

     ​

     * Cent OS安装命令
       1. 安装相关的`python`全家桶

       ```yum install python-pip python-devel python-distribute libxml2 libxml2-devel python-lxml libxslt libxslt-devel openssl openssl-devel -y```

       ​2.  更新`pip`源

       ​```pip install --upgrade pip```

       ​3.安装`scrapy`框架

       ​```pip install scrapy```

     ​

     * MAC OS是自带python 2.7的，而目前Scrapy也只是在python 2.7版本较为稳定，所以你不需要再安装或者配置python环境了，只需要按步骤安装好Scrapy就好了。

       1. 使用`wget`下载命令下载`pip`包

       ```wget https://bootstrap.pypa.io/get-pip.py```

       2. 安装`pip`

       ```sudo python get-pip.py```

       3. 更新`pip`源

       ```pip install --upgrade pip```

       4. 安装`Scrapy`框架

       ```pip install scrapy```

       ​

       > 注意 : 安装完成pip之后，默认的时官方源可能会被我大天朝的长城K.O.掉，我们需要对pip源进行修改，使用咱们国内的源(如果在在后面的pip install XX的过程出现错误,那可能就是pip源有问题,需要配置国内的pip源)

   * 安装`iPython`   (可选)

     > *IPython*是一个交互式计算系统。主要包含三个组件：增加的交互式 “Python shell”，解耦的双过程通信模型，交互式并行计算的架构。支持变量自动补全。

     * `sudo pip install iPython`

     ![](http://p0kzdnfmg.bkt.clouddn.com/17-12-29/36742008.jpg)

   * 在Chorme浏览器上安装`xpath helper `插件

     [安装教程参考](https://www.jianshu.com/p/9651c0079e5a)

   ​

   ​





## Scrapy Shell

Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。

如果安装了 IPython ，Scrapy终端将使用 IPython (替代标准Python终端)。 IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。（推荐安装IPython）

当我们在命令行键入`scrapy`

![](http://p0kzdnfmg.bkt.clouddn.com/17-12-29/65425744.jpg)

我们就根据这张图,简单介绍一下scrapy常用命令:





### Scrapy Shell命令

```scrapy shell https://tieba.baidu.com/p/5422537709```

![](http://p0kzdnfmg.bkt.clouddn.com/17-12-29/5380431.jpg)

Scrapy Shell根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象，以及 `Selector 对象 (对HTML及XML内容)`。

- 当shell载入后，将得到一个包含response数据的本地 response 变量，输入 `response.body`将输出response的包体，输出 `response.headers` 可以看到response的包头。
- 输入 `response.selector` 时， 将获取到一个response 初始化的类 Selector 的对象，此时可以通过使用 `response.selector.xpath()`或`response.selector.css()` 来对 response 进行查询。
- Scrapy也提供了一些快捷方式, 例如 `response.xpath()`或`response.css()`

1. ​

## Scrapy的使用 1







## scrapy的使用 2




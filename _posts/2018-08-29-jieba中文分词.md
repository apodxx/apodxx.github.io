---
layout: post
title: jieba中文分词
date: 2018-08-29
tag: 机器学习
---

# 使用jieba对中文分词

> 我们在文本进行数据挖掘时，往往第一步便是对文本进行分词处理，分词，顾名思义就是将一篇文章分成若干词语，进而进行统计分析，英文以空格便能很简便的对文字进行分词，而中文则用到了jieba来进行分词


### 安装

方法一：

1. 我们先访问Pypi网站下载安装包：[传送门](https://pypi.python.org/pypi/jieba)
2. 解压
3. 在该目录下按住`shift`，右键，在此处打开命令行（Linux和mac无视）
4. 执行命令`python setyp.py install `

方法二：直接`pip install pip`

最后，

在python文件中，开头导入jieba，如果没有出错，则表示安装成功

	import jieba

### 分词模式
	

* 精确模式：试图将句子最精确地切开，适合文本分析； 
* 全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义问题； 
* 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。


----------



	
	import jieba
	sentence = "我喜欢上海东方明珠"
	# cut(句子变量，分词模式（1 全模式，2精准模式(默认)）)
	# 1 全模式
	wordArr = jieba.cut(sentence, cut_all=True)
	for word in wordArr:
	    print(word)

输出结果：

		我
		喜欢
		上海
		上海东方
		海东
		东方
		东方明珠
		方明
		明珠
			
----------

	# 2精准模式(默认)
	wordArr1 = jieba.cut(sentence, cut_all=False)
	for word in wordArr1:
	    print(word)

输出结果：

	我
	喜欢
	上海
	东方明珠

----------

	#  3搜索引擎分词模式
	wordArr2 = jieba.cut_for_search(sentence)
	for word in wordArr2:
	    print(word)
	
输出结果：

	我
	喜欢
	上海
	东方
	方明
	明珠
	东方明珠


### 返回词语的位置


	import jieba.analyse
	
	words = '我喜欢上海东方明珠'
	tag = jieba.analyse.extract_tags(words, 2)
	print(tag)
	print("------------")
	
	# 返回词语的位置(精准模式)
	wordOrder = jieba.tokenize(sentence)
	for word in wordOrder:
	    print(word)
	print("------------")
	
	# 返回词语的位置（搜索引擎模式）
	wordOrder = jieba.tokenize(sentence, mode="search")
	for word in wordOrder:
	    print(word)
	
	print("------------")

输出结果：

	['东方明珠', '喜欢']
	------------
	('我', 0, 1)
	('喜欢', 1, 3)
	('上海', 3, 5)
	('东方明珠', 5, 9)
	------------
	('我', 0, 1)
	('喜欢', 1, 3)
	('上海', 3, 5)
	('东方', 5, 7)
	('方明', 6, 8)
	('明珠', 7, 9)
	('东方明珠', 5, 9)
	------------

### 输出词语的词性



	import jieba.posseg
	# 属性word表示词语，flag表示词性

	sen = "医院白求恩一天工作10个小时"
	wordType = jieba.posseg.cut(sen)
	for item in wordType:
	    print(item.word + "--" + item.flag)

结果输出

	医院--n
	白求恩--nr
	一天--m
	工作--vn
	10--m
	个--m
	小时--n


flag字母表示意义

    a:形容词
    c:连词
    d:副词
    e:叹词
    f:方位词
    i:成语
    m:数词
    n:名词
    nr:人名
    ns:地名
    nt:机构团体
    nz:其他专有词语
    p:介词
    r:代词
    t:时间
    u:助词
    v:动词
    vn:动名词
    w:标点符号
    un:未知词语



### 添加自定义的词典

由于我的python是使用anacoada安装的，可以找到安装好的jieba的目录`E:/Users/Administrator/Anaconda3/Lib/site-packages/jieba/`,其中`dict.txt`就是jieba预设的字典，
如果你对jieba的分词不满意，还可以加上在自己写好的自定义词典,
`dict2.txt`是我自己写的自定义词典

![](http://p0kzdnfmg.bkt.clouddn.com/18-9-5/59978774.jpg)

![](http://p0kzdnfmg.bkt.clouddn.com/18-9-5/17613932.jpg)


PS：我没打广告哦，随便写写

----------

	# 加载自定义词典（加载到了内存中）
	jieba.load_userdict("E:/Users/Administrator/Anaconda3/Lib/site-packages/jieba/dict2.txt")
	sentence2 = "天善智能传智播客文都教育"
	wordArr3 = jieba.posseg.cut(sentence2)
	for w in wordArr3:
	    print(w.word + "--" + w.flag)

输出结果：

	天善智能--nt
	传智播客--nt
	文都教育--nt

	

[github代码](https://github.com/apodxx/jieba_participle)

